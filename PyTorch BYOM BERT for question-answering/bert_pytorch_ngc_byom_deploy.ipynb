{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying BERT with Pytorch and NGC\n",
    "Ever wondered how to efficiently deploy massive transformer networks in the cloud? It is surprisingly easy using AWS and Nvidia GPU Cloud (NGC) containers. For this tutorial, we are going to download a BERT base question answering model trained on the Stanford Question Answering Dataset and walk through the steps necessary to deploy it to a Sagemaker endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step will be to download the pretrained question answering model from NGC as well as a repo containing all of the necessary code for deploying this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-08 14:59:02--  https://api.ngc.nvidia.com/v2/models/nvidia/bert_base_pyt_amp_ckpt_squad_qa1_1/versions/1/files/bert_base_qa.pt\n",
      "Resolving api.ngc.nvidia.com (api.ngc.nvidia.com)... 54.186.237.130, 52.38.124.212\n",
      "Connecting to api.ngc.nvidia.com (api.ngc.nvidia.com)|54.186.237.130|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 \n",
      "Location: https://s3.us-west-2.amazonaws.com/prod-model-registry-ngc-bucket/org/nvidia/models/bert_base_pyt_amp_ckpt_squad_qa1_1/versions/1/files/bert_base_qa.pt?response-content-disposition=attachment%3B%20filename%3D%22bert_base_qa.pt%22&response-content-type=application%2Foctet-stream&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEJ7%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJHMEUCIDRJWIlUPM42x82sh2gMiQ3qtrXC0%2BHox5iW8%2BSWIzstAiEAxnRLNpg4hSiaQvd6qyrpK2UpQylYqMDFAQb9mDSyLOYqvQMI1%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARACGgw3ODkzNjMxMzUwMjciDGsSC9JvDkjynWucLSqRA2HOW66FT4N862fGJoaVEXrh81zs6sJJxQEheUJN4Fa3q8JI7nQxmFBUXs61PtwLs2Uc%2BmFc%2F1uiAEHvhCjTX9%2F3jKDsi5014ZM5HoFwKhr%2BFq1QHwWhAU921WvFvAMnFVQG6yX%2BFXPLCI9yWWzQIpaiYZtipu%2Fqjy6bLuW8rktPKdGtA7do8vvTzVLt2Dd85Yob4jaz4IJo91nIEH5b%2BVzOX9dgpbhcFrylIqmGTEfOdyOWikF3fxbeL3GaJJh1zmr%2BD4mR3vsbDB5JO68Uyx0u3ax%2BCm7mrGUVv5oTv2wCOcimJ%2B8dw%2Bda%2B%2FxBQ56WpUkkfQzlsXSSmjBsll7OiAQEcDy8nqyAnBHIYYxSinMV%2F1ANa7T54FQ%2FkvVrlOc9SSF5Qy0VtYLj%2Bla3vU5eUCb1uB5d4yW463GKmy3QavoXVYuH6rSfgrzwnNB6rOBiYVCE014Q5IcY8336nSmRP9krzwRtyPm%2FQs5FD%2Br9lpG1REUrg2W3%2FLDAWcr91oURevbTY2RXFw8eR5MNe7IKd5khMPDB1fUFOusBE%2B8VrX%2F8rX%2FXYbBwr4XjTY6Az64ZxCv3kfStwaT1A0spV3%2BJxJgQwkghbSdL0TtseTfyIyXF%2F7G96N%2F%2BDAG1vN3PL017UpKd6Pwc4iEUh%2F%2BbjYL1vkzW%2BfBhbc%2BrwZVuBUi5RqO4kCLA5QNEkaGoBxaRaxFb%2FCAw3HnsBd7oYxJSArKsIqUO9PGMbgAk3CRP08coNgp9rKqiEWr%2Bl5Xb4os5rKsR4EZoJ73ZutVbQPzbaeH0XpIZ%2FZnWIDfeIYPhzU%2F%2FJbrUpiIiZ8ECsvYJw36dGaA9DLrQT0XScW7xy7G3O6crBO5zTNHxbQ%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20200508T145902Z&X-Amz-SignedHeaders=host&X-Amz-Expires=3600&X-Amz-Credential=ASIA3PSNVSIZVV6YQSVC%2F20200508%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Signature=ebfc68e57a536a19fea1648975134a5815066312dc0592e99530e4d0f9e5fd9f [following]\n",
      "--2020-05-08 14:59:02--  https://s3.us-west-2.amazonaws.com/prod-model-registry-ngc-bucket/org/nvidia/models/bert_base_pyt_amp_ckpt_squad_qa1_1/versions/1/files/bert_base_qa.pt?response-content-disposition=attachment%3B%20filename%3D%22bert_base_qa.pt%22&response-content-type=application%2Foctet-stream&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEJ7%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJHMEUCIDRJWIlUPM42x82sh2gMiQ3qtrXC0%2BHox5iW8%2BSWIzstAiEAxnRLNpg4hSiaQvd6qyrpK2UpQylYqMDFAQb9mDSyLOYqvQMI1%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARACGgw3ODkzNjMxMzUwMjciDGsSC9JvDkjynWucLSqRA2HOW66FT4N862fGJoaVEXrh81zs6sJJxQEheUJN4Fa3q8JI7nQxmFBUXs61PtwLs2Uc%2BmFc%2F1uiAEHvhCjTX9%2F3jKDsi5014ZM5HoFwKhr%2BFq1QHwWhAU921WvFvAMnFVQG6yX%2BFXPLCI9yWWzQIpaiYZtipu%2Fqjy6bLuW8rktPKdGtA7do8vvTzVLt2Dd85Yob4jaz4IJo91nIEH5b%2BVzOX9dgpbhcFrylIqmGTEfOdyOWikF3fxbeL3GaJJh1zmr%2BD4mR3vsbDB5JO68Uyx0u3ax%2BCm7mrGUVv5oTv2wCOcimJ%2B8dw%2Bda%2B%2FxBQ56WpUkkfQzlsXSSmjBsll7OiAQEcDy8nqyAnBHIYYxSinMV%2F1ANa7T54FQ%2FkvVrlOc9SSF5Qy0VtYLj%2Bla3vU5eUCb1uB5d4yW463GKmy3QavoXVYuH6rSfgrzwnNB6rOBiYVCE014Q5IcY8336nSmRP9krzwRtyPm%2FQs5FD%2Br9lpG1REUrg2W3%2FLDAWcr91oURevbTY2RXFw8eR5MNe7IKd5khMPDB1fUFOusBE%2B8VrX%2F8rX%2FXYbBwr4XjTY6Az64ZxCv3kfStwaT1A0spV3%2BJxJgQwkghbSdL0TtseTfyIyXF%2F7G96N%2F%2BDAG1vN3PL017UpKd6Pwc4iEUh%2F%2BbjYL1vkzW%2BfBhbc%2BrwZVuBUi5RqO4kCLA5QNEkaGoBxaRaxFb%2FCAw3HnsBd7oYxJSArKsIqUO9PGMbgAk3CRP08coNgp9rKqiEWr%2Bl5Xb4os5rKsR4EZoJ73ZutVbQPzbaeH0XpIZ%2FZnWIDfeIYPhzU%2F%2FJbrUpiIiZ8ECsvYJw36dGaA9DLrQT0XScW7xy7G3O6crBO5zTNHxbQ%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20200508T145902Z&X-Amz-SignedHeaders=host&X-Amz-Expires=3600&X-Amz-Credential=ASIA3PSNVSIZVV6YQSVC%2F20200508%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Signature=ebfc68e57a536a19fea1648975134a5815066312dc0592e99530e4d0f9e5fd9f\n",
      "Resolving s3.us-west-2.amazonaws.com (s3.us-west-2.amazonaws.com)... 52.218.242.160\n",
      "Connecting to s3.us-west-2.amazonaws.com (s3.us-west-2.amazonaws.com)|52.218.242.160|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 438004726 (418M) [application/octet-stream]\n",
      "Saving to: ‘bert_base_qa.pt’\n",
      "\n",
      "bert_base_qa.pt     100%[===================>] 417.71M  1.31MB/s    in 4m 21s  \n",
      "\n",
      "2020-05-08 15:03:24 (1.60 MB/s) - ‘bert_base_qa.pt’ saved [438004726/438004726]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://api.ngc.nvidia.com/v2/models/nvidia/bert_base_pyt_amp_ckpt_squad_qa1_1/versions/1/files/bert_base_qa.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import torch\n",
    "import os, tarfile, json\n",
    "import time, datetime\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import estimator, PyTorchModel, PyTorchPredictor\n",
    "from sagemaker.utils import name_from_base\n",
    "import boto3\n",
    "from modeling import BertForQuestionAnswering, BertConfig, WEIGHTS_NAME, CONFIG_NAME\n",
    "from tokenization import (BasicTokenizer, BertTokenizer, whitespace_tokenize)\n",
    "from types import SimpleNamespace\n",
    "from helper_funcs import *\n",
    "from file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket() # can replace with your own S3 bucket\n",
    "prefix = 'bert_pytorch_ngc'\n",
    "runtime_client = boto3.client('runtime.sagemaker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model locally\n",
    "Before deploying everything to an endpoint, let's run through how the model works and run inference locally.\n",
    "We are first going to set some variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the vocabulary file\n",
    "vocab_file='vocab'\n",
    "\n",
    "# set variables that limit the maximum length of the context, query, and answer\n",
    "max_seq_length, max_query_length, n_best_size, max_answer_length, null_score_diff_threshold= 384, 64, 1, 30, -11.0\n",
    "do_lower_case, can_give_negative_answer=True, True\n",
    "\n",
    "# initialize our tokenizer\n",
    "tokenizer = BertTokenizer(vocab_file, do_lower_case=True, max_len=512)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30528, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense_act): LinearActivation(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense_act): LinearActivation(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense_act): LinearActivation(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense_act): LinearActivation(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense_act): LinearActivation(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense_act): LinearActivation(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense_act): LinearActivation(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense_act): LinearActivation(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense_act): LinearActivation(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense_act): LinearActivation(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense_act): LinearActivation(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense_act): LinearActivation(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense_act): LinearActivation(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load a model configuration\n",
    "config = BertConfig.from_json_file('bert_config.json')\n",
    "\n",
    "# set up our model architecture\n",
    "model = BertForQuestionAnswering(config)\n",
    "\n",
    "# load our weights\n",
    "model.load_state_dict(torch.load('bert_base_qa.pt', map_location=device)[\"model\"])\n",
    "\n",
    "# set our model to evaluation mode for inference\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out inference! For question answering, we need to supply a context statement that our model can \"study\" in order to answer the question. We then ask it a question about the context statement. Feel free to change the context statement and question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who loves Steve? : Danielle\n"
     ]
    }
   ],
   "source": [
    "# specify how many answers to return, here we are going to take the top answer only.\n",
    "n_best_size=1\n",
    "context='Danielle is a girl who really loves her cat, Steve. Steve is a large cat with a very furry belly. He gets very excited by the prospect of eating chicken covered in gravy.'\n",
    "question='who loves Steve?'  # 'What kind of food does Steve like?'\n",
    "\n",
    "# preprocessing\n",
    "# split the context into tokens\n",
    "doc_tokens = context.split()\n",
    "# tokenize our query \n",
    "query_tokens = tokenizer.tokenize(question)\n",
    "# generate features to feed to the model\n",
    "feature = preprocess_tokenized_text(doc_tokens, \n",
    "                                    query_tokens, \n",
    "                                    tokenizer, \n",
    "                                    max_seq_length=max_seq_length, \n",
    "                                    max_query_length=max_query_length)\n",
    "tensors_for_inference, tokens_for_postprocessing = feature\n",
    "\n",
    "input_ids = torch.tensor(tensors_for_inference.input_ids, dtype=torch.long).unsqueeze(0)\n",
    "segment_ids = torch.tensor(tensors_for_inference.segment_ids, dtype=torch.long).unsqueeze(0)\n",
    "input_mask = torch.tensor(tensors_for_inference.input_mask, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "# load tensors to device\n",
    "input_ids = input_ids.to(device)\n",
    "input_mask = input_mask.to(device)\n",
    "segment_ids = segment_ids.to(device)\n",
    "\n",
    "# run inference\n",
    "with torch.no_grad():\n",
    "    start_logits, end_logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "# post-processing\n",
    "start_logits = start_logits[0].detach().cpu().tolist()\n",
    "end_logits = end_logits[0].detach().cpu().tolist()\n",
    "# convert logits back to English\n",
    "answer = get_predictions(doc_tokens, tokens_for_postprocessing, \n",
    "                         start_logits, end_logits, n_best_size, \n",
    "                         max_answer_length, do_lower_case, \n",
    "                         can_give_negative_answer, \n",
    "                         null_score_diff_threshold)\n",
    "\n",
    "# print result\n",
    "print(f'{question} : {answer[0][\"text\"]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the pretrained model\n",
    "Now that you've gotten a chance to play with the model locally, let's deploy it to an endpoint! In order to deploy BERT to a sagemaker endpoint, we need to save the model as a tarball. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model as a tarball\n",
    "with tarfile.open('bert.tar.gz', 'w:gz') as f:\n",
    "    f.add('bert_base_qa.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the model\n",
    "Once we have saved our model we then upload to our S3 bucket where our Docker container can access it. We use transform_script.py to define how we load our model, handle our input data, perform inference, and pass our results back to the requester. \n",
    "\n",
    "Sagemaker has predefined functions for all of these operations aside from importing the model, however, for our specific case we are passing in multiple arrays as input (our question and our provided context). This means we need to specify custom functions for our input data and making predictions. These functions are named input_fn and predict_fn inside of transform_script.py. To learn more about how to deploy PyTorch models in sagemaker see the following documentation:\n",
    "\n",
    "https://sagemaker.readthedocs.io/en/stable/using_pytorch.html#deploy-pytorch-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload model data to S3\n",
    "model_data = sagemaker_session.upload_data(path='bert.tar.gz',\n",
    "                                           bucket=bucket,\n",
    "                                           key_prefix =os.path.join(prefix, 'model'))\n",
    "\n",
    "# instantiate model\n",
    "torch_model = PyTorchModel(model_data=model_data,\n",
    "                           role=role,\n",
    "                          entry_point='transform_script.py',\n",
    "                          framework_version='1.4.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model\n",
    "Now that we have defined our model we can deploy it to an endpoint. We will need to give our endpoint a name, determine how many instances we want to run our endpoint, and the instance types. Here we are deploying this model to a g4dn instance that utilizes a Nvidia T4 card for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "# deploy endpoint, this part may take a bit\n",
    "endpoint_name = f'bert-endpoint-{datetime.datetime.fromtimestamp(time.time()).strftime(\"%c\").replace(\" \",\"-\").replace(\":\",\"-\")}'\n",
    "bert_end = torch_model.deploy(instance_type='ml.g4dn.8xlarge', initial_instance_count=1, \n",
    "                              endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Predictions\n",
    "For question answering, we pass in a context statement for the model to read and then we ask it a question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who loves Steve? : Danielle\n",
      "inference took: 0.0555 seconds\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "n_best_size=3\n",
    "context='Danielle is a girl who really loves her cat, Steve. Steve is a large cat with a very furry belly. He gets very excited by the prospect of eating chicken covered in gravy.'\n",
    "question='who loves Steve?'  # 'What kind of food does Steve like?'\n",
    "doc_tokens = context.split()\n",
    "query_tokens = tokenizer.tokenize(question)\n",
    "feature = preprocess_tokenized_text(doc_tokens, \n",
    "                                    query_tokens, \n",
    "                                    tokenizer, \n",
    "                                    max_seq_length=max_seq_length, \n",
    "                                    max_query_length=max_query_length)\n",
    "tensors_for_inference, tokens_for_postprocessing = feature\n",
    "\n",
    "input_ids = np.array(tensors_for_inference.input_ids, dtype=np.int64)\n",
    "segment_ids = np.array(tensors_for_inference.segment_ids, dtype=np.int64)\n",
    "input_mask = np.array(tensors_for_inference.input_mask, dtype=np.int64)   \n",
    "\n",
    "payload = np.concatenate([np.expand_dims(input_ids, axis=0), np.expand_dims(segment_ids, axis=0), np.expand_dims(input_mask, axis=0)])\n",
    "try:\n",
    "    response = bert_end.predict(payload.tobytes(), initial_args={'ContentType':'application/x-npy'}) \n",
    "except:\n",
    "    print('using invoke_endpoint directly')\n",
    "    response = runtime_client.invoke_endpoint(EndpointName='bert-endpoint-Thu-May--7-00-39-48-2020', #endpoint_name,\n",
    "                                           ContentType='application/x-npy',\n",
    "                                           Body=payload.tobytes())\n",
    "    response = eval(response['Body'].read().decode('utf-8'))\n",
    "answer = get_predictions(doc_tokens, tokens_for_postprocessing, \n",
    "                         response[0], response[1], n_best_size, \n",
    "                         max_answer_length, do_lower_case, \n",
    "                         can_give_negative_answer, \n",
    "                         null_score_diff_threshold)\n",
    "\n",
    "# print result\n",
    "print(f'{question} : {answer[0][\"text\"]}')\n",
    "print(f'inference took: {round(time.time()-t,4)} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_end.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
