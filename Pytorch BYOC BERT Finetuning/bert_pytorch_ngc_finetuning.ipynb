{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning PyTorch BERT with NGC\n",
    "The BERT family of models are a powerful set of natural language understanding models based on the transformer architecture from the paper Attention Is All You Need, which you can find here:  https://arxiv.org/abs/1706.03762\n",
    "\n",
    "These models work by running unsupervised pre-training on massive sets of text data. This process requires an enormous amount of time and compute. Luckily for us, BERT models are built for transfer learning. BERT models are able to be finetuned to perform many different NLU tasks like question answering, sentiment analysis, document summarization, and more.\n",
    "\n",
    "For this tutorial, we are going to download a BERT base model and finetune this model on the Stanford Question Answering Dataset and walk through the steps necessary to deploy it to a Sagemaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://api.ngc.nvidia.com/v2/models/nvidia/bert_base_pyt_amp_ckpt_pretraining_lamb/versions/1/files/bert_base.pt -O DeepLearningExamples/PyTorch/LanguageModeling/BERT/bert_base.pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import math\n",
    "import torch\n",
    "import os, tarfile, json\n",
    "import time, datetime\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import estimator, PyTorchModel, PyTorchPredictor, PyTorch\n",
    "from sagemaker.utils import name_from_base\n",
    "import boto3\n",
    "from file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "from modeling import BertForQuestionAnswering, BertConfig, WEIGHTS_NAME, CONFIG_NAME\n",
    "from tokenization import (BasicTokenizer, BertTokenizer, whitespace_tokenize)\n",
    "from types import SimpleNamespace\n",
    "from helper_funcs import *\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = 'privisaa-bucket-virginia' # sagemaker_session.default_bucket() # can replace with your own S3 bucket\n",
    "prefix = 'bert_pytorch_ngc'\n",
    "runtime_client = boto3.client('runtime.sagemaker')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create our training docker container\n",
    "\n",
    "Now we are going to create a custom docker container based on the NGC Bert container and push it to AWS Elastic Container Registry (ECR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Sending build context to Docker daemon  1.578MB\n",
      "Step 1/15 : ARG FROM_IMAGE_NAME=nvcr.io/nvidia/pytorch:19.12-py3\n",
      "Step 2/15 : FROM ${FROM_IMAGE_NAME}\n",
      " ---> be021446e08c\n",
      "Step 3/15 : RUN apt-get update && apt-get install -y pbzip2 pv bzip2 cabextract nginx wget\n",
      " ---> Using cache\n",
      " ---> 147b590e3ef2\n",
      "Step 4/15 : ENV BERT_PREP_WORKING_DIR /workspace/bert/data\n",
      " ---> Using cache\n",
      " ---> c3c6c01540f5\n",
      "Step 5/15 : WORKDIR /workspace\n",
      " ---> Using cache\n",
      " ---> 7dc784098462\n",
      "Step 6/15 : RUN git clone https://github.com/attardi/wikiextractor.git\n",
      " ---> Using cache\n",
      " ---> 013f76c5ec7b\n",
      "Step 7/15 : RUN git clone https://github.com/soskek/bookcorpus.git\n",
      " ---> Using cache\n",
      " ---> 4b429d99ab2f\n",
      "Step 8/15 : WORKDIR /workspace/bert\n",
      " ---> Using cache\n",
      " ---> 1a999459ad05\n",
      "Step 9/15 : RUN pip install --upgrade --no-cache-dir pip  && pip install --no-cache-dir  gevent flask pathlib gunicorn tqdm boto3 requests six ipdb h5py html2text nltk progressbar onnxruntime git+https://github.com/NVIDIA/dllogger\n",
      " ---> Using cache\n",
      " ---> 759b949540fd\n",
      "Step 10/15 : RUN apt-get install -y iputils-ping\n",
      " ---> Using cache\n",
      " ---> 7b7450f030b7\n",
      "Step 11/15 : ENV PATH=\"/workspace/bert:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> a98ddb4f6663\n",
      "Step 12/15 : RUN mkdir /opt/ml && mkdir /opt/ml/input && mkdir /opt/ml/output && mkdir /opt/ml/model && mkdir /opt/ml/input/config\n",
      " ---> Using cache\n",
      " ---> f7832e165525\n",
      "Step 13/15 : COPY . .\n",
      " ---> 207a701e3447\n",
      "Step 14/15 : RUN chmod +x /workspace/bert/train\n",
      " ---> Running in ac8540ba6700\n",
      "Removing intermediate container ac8540ba6700\n",
      " ---> ba998d9a3adf\n",
      "Step 15/15 : RUN chmod +x /workspace/bert/serve\n",
      " ---> Running in 4756e37e11dc\n",
      "Removing intermediate container 4756e37e11dc\n",
      " ---> 994f1bae86e9\n",
      "Successfully built 994f1bae86e9\n",
      "Successfully tagged bert-ngc-torch-train:latest\n",
      "The push refers to repository [209419068016.dkr.ecr.us-east-1.amazonaws.com/bert-ngc-torch-train]\n",
      "c10190f15d35: Preparing\n",
      "1ec4a2fa0e3b: Preparing\n",
      "f7dcdd647d1e: Preparing\n",
      "c47ea7d519dd: Preparing\n",
      "98787e647c55: Preparing\n",
      "50734da9c790: Preparing\n",
      "aee799061b46: Preparing\n",
      "72703cc32079: Preparing\n",
      "86e76f54081b: Preparing\n",
      "87f8c1b909bb: Preparing\n",
      "fddcdacefca4: Preparing\n",
      "6c9a5f2bcdc9: Preparing\n",
      "97f77c2bf551: Preparing\n",
      "0ad644802067: Preparing\n",
      "0988452d60ad: Preparing\n",
      "22b2247d543c: Preparing\n",
      "5130ccfce7b2: Preparing\n",
      "005c189102b1: Preparing\n",
      "b2541313126e: Preparing\n",
      "7a631d1de8a8: Preparing\n",
      "563ea1e7989f: Preparing\n",
      "05b737b70379: Preparing\n",
      "87f8c1b909bb: Waiting\n",
      "72703cc32079: Waiting\n",
      "461c94146b25: Preparing\n",
      "86e76f54081b: Waiting\n",
      "50734da9c790: Waiting\n",
      "89f14d452cdc: Preparing\n",
      "6c9a5f2bcdc9: Waiting\n",
      "221c639fb572: Preparing\n",
      "97f77c2bf551: Waiting\n",
      "aee799061b46: Waiting\n",
      "094a55ed8561: Preparing\n",
      "fddcdacefca4: Waiting\n",
      "c727ce4f07f0: Preparing\n",
      "5f4f32dbd55d: Preparing\n",
      "0988452d60ad: Waiting\n",
      "577dd6013185: Preparing\n",
      "22b2247d543c: Waiting\n",
      "0ad644802067: Waiting\n",
      "78c62c90c01c: Preparing\n",
      "5130ccfce7b2: Waiting\n",
      "b2541313126e: Waiting\n",
      "6d5f1e49ad99: Preparing\n",
      "005c189102b1: Waiting\n",
      "89f14d452cdc: Waiting\n",
      "96a6eb08694f: Preparing\n",
      "05b737b70379: Waiting\n",
      "7a631d1de8a8: Waiting\n",
      "b0404397b1f6: Preparing\n",
      "461c94146b25: Waiting\n",
      "c558708f95ac: Preparing\n",
      "577dd6013185: Waiting\n",
      "5f4f32dbd55d: Waiting\n",
      "c727ce4f07f0: Waiting\n",
      "221c639fb572: Waiting\n",
      "c003f7d80d34: Preparing\n",
      "094a55ed8561: Waiting\n",
      "6d5f1e49ad99: Waiting\n",
      "9f34fb2ba40d: Preparing\n",
      "b0404397b1f6: Waiting\n",
      "a9268194e7cd: Preparing\n",
      "c003f7d80d34: Waiting\n",
      "8e893f1677ca: Preparing\n",
      "9f34fb2ba40d: Waiting\n",
      "a9268194e7cd: Waiting\n",
      "cc0b30658be2: Preparing\n",
      "77db1fc0f8a5: Preparing\n",
      "5cb994f962f2: Preparing\n",
      "8e893f1677ca: Waiting\n",
      "045b335db3d7: Preparing\n",
      "cc0b30658be2: Waiting\n",
      "ffa33d177975: Preparing\n",
      "c7b9adf34c73: Preparing\n",
      "77db1fc0f8a5: Waiting\n",
      "e385f28a9371: Preparing\n",
      "aa98ab4614ff: Preparing\n",
      "ffa33d177975: Waiting\n",
      "045b335db3d7: Waiting\n",
      "aa42716fcc2f: Preparing\n",
      "5cb994f962f2: Waiting\n",
      "2fcfd47c7b80: Preparing\n",
      "aa98ab4614ff: Waiting\n",
      "e385f28a9371: Waiting\n",
      "40f9283f52a3: Preparing\n",
      "21fa43656dc2: Preparing\n",
      "2fcfd47c7b80: Waiting\n",
      "519d10164fb4: Preparing\n",
      "b4e68a0c3294: Preparing\n",
      "40f9283f52a3: Waiting\n",
      "4fc18f064f08: Preparing\n",
      "21fa43656dc2: Waiting\n",
      "7a0c9aa36cb6: Preparing\n",
      "b4e68a0c3294: Waiting\n",
      "911e33e3bdb5: Preparing\n",
      "a12854a1091b: Preparing\n",
      "d489d7212cf0: Preparing\n",
      "911e33e3bdb5: Waiting\n",
      "7a0c9aa36cb6: Waiting\n",
      "a12854a1091b: Waiting\n",
      "d9d8412a92db: Preparing\n",
      "d489d7212cf0: Waiting\n",
      "7cdd75a5beec: Preparing\n",
      "5f514c1078a9: Preparing\n",
      "d9d8412a92db: Waiting\n",
      "7cdd75a5beec: Waiting\n",
      "52d0beb8ba85: Preparing\n",
      "bef829be1d9b: Preparing\n",
      "47a60ca21bc2: Preparing\n",
      "43e75c4a74b7: Preparing\n",
      "5f514c1078a9: Waiting\n",
      "52d0beb8ba85: Waiting\n",
      "d5a82cd15754: Preparing\n",
      "47a60ca21bc2: Waiting\n",
      "43e75c4a74b7: Waiting\n",
      "e0b3afb09dc3: Preparing\n",
      "6c01b5a53aac: Preparing\n",
      "d5a82cd15754: Waiting\n",
      "2c6ac8e5063e: Preparing\n",
      "cc967c529ced: Preparing\n",
      "6c01b5a53aac: Waiting\n",
      "e0b3afb09dc3: Waiting\n",
      "2c6ac8e5063e: Waiting\n",
      "c47ea7d519dd: Layer already exists\n",
      "98787e647c55: Layer already exists\n",
      "50734da9c790: Layer already exists\n",
      "aee799061b46: Layer already exists\n",
      "86e76f54081b: Layer already exists\n",
      "72703cc32079: Layer already exists\n",
      "fddcdacefca4: Layer already exists\n",
      "6c9a5f2bcdc9: Layer already exists\n",
      "87f8c1b909bb: Layer already exists\n",
      "97f77c2bf551: Layer already exists\n",
      "0ad644802067: Layer already exists\n",
      "0988452d60ad: Layer already exists\n",
      "22b2247d543c: Layer already exists\n",
      "5130ccfce7b2: Layer already exists\n",
      "005c189102b1: Layer already exists\n",
      "b2541313126e: Layer already exists\n",
      "7a631d1de8a8: Layer already exists\n",
      "563ea1e7989f: Layer already exists\n",
      "461c94146b25: Layer already exists\n",
      "05b737b70379: Layer already exists\n",
      "89f14d452cdc: Layer already exists\n",
      "1ec4a2fa0e3b: Pushed\n",
      "221c639fb572: Layer already exists\n",
      "094a55ed8561: Layer already exists\n",
      "5f4f32dbd55d: Layer already exists\n",
      "c727ce4f07f0: Layer already exists\n",
      "577dd6013185: Layer already exists\n",
      "6d5f1e49ad99: Layer already exists\n",
      "78c62c90c01c: Layer already exists\n",
      "96a6eb08694f: Layer already exists\n",
      "c558708f95ac: Layer already exists\n",
      "b0404397b1f6: Layer already exists\n",
      "c10190f15d35: Pushed\n",
      "c003f7d80d34: Layer already exists\n",
      "a9268194e7cd: Layer already exists\n",
      "9f34fb2ba40d: Layer already exists\n",
      "8e893f1677ca: Layer already exists\n",
      "cc0b30658be2: Layer already exists\n",
      "77db1fc0f8a5: Layer already exists\n",
      "045b335db3d7: Layer already exists\n",
      "5cb994f962f2: Layer already exists\n",
      "ffa33d177975: Layer already exists\n",
      "e385f28a9371: Layer already exists\n",
      "c7b9adf34c73: Layer already exists\n",
      "aa98ab4614ff: Layer already exists\n",
      "aa42716fcc2f: Layer already exists\n",
      "f7dcdd647d1e: Pushed\n",
      "2fcfd47c7b80: Layer already exists\n",
      "40f9283f52a3: Layer already exists\n",
      "21fa43656dc2: Layer already exists\n",
      "b4e68a0c3294: Layer already exists\n",
      "519d10164fb4: Layer already exists\n",
      "4fc18f064f08: Layer already exists\n",
      "7a0c9aa36cb6: Layer already exists\n",
      "911e33e3bdb5: Layer already exists\n",
      "a12854a1091b: Layer already exists\n",
      "d9d8412a92db: Layer already exists\n",
      "5f514c1078a9: Layer already exists\n",
      "d489d7212cf0: Layer already exists\n",
      "7cdd75a5beec: Layer already exists\n",
      "bef829be1d9b: Layer already exists\n",
      "52d0beb8ba85: Layer already exists\n",
      "43e75c4a74b7: Layer already exists\n",
      "47a60ca21bc2: Layer already exists\n",
      "d5a82cd15754: Layer already exists\n",
      "e0b3afb09dc3: Layer already exists\n",
      "2c6ac8e5063e: Layer already exists\n",
      "6c01b5a53aac: Layer already exists\n",
      "cc967c529ced: Layer already exists\n",
      "latest: digest: sha256:4f6ba258032856409a0dedd6c51473bf54c05dc44ca2d6dc154ee3d1b1ef8831 size: 14801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: line 5: cd: DeepLearningExamples/PyTorch/LanguageModeling/BERT: No such file or directory\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=bert-ngc-torch-train\n",
    "\n",
    "cd DeepLearningExamples/PyTorch/LanguageModeling/BERT\n",
    "\n",
    "chmod +x train\n",
    "chmod +x serve\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-east-1}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build  -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "# some kind of security auth issue with pushing this to ecr, not authorized to perform ecr:InitiateLayerUpload\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the model\n",
    "\n",
    "Now we are going to instantiate our model, here we are going to specify our hyperparameters for training as well as the number of GPUs we are going to use. The ml.p3.16xlarge instances contain 8 V100 volta GPUs, making them ideal for heavy duty deep learning training. \n",
    "\n",
    "Once we have set our hyperparameters, we will instantiate a Sagemaker Estimator that we will use to run our training job. We specify the Docker image we just pushed to ECR as well as an entrypoint giving instructions for what operations our container should perform when it starts up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set our hyperparameters\n",
    "hyperparameters = {'bert_model': 'bert-base-uncased',  'num_train_epochs': 1, \n",
    "                   'vocab_file': '/workspace/bert/data/bert_vocab.txt',\n",
    "                   'config_file':'/workspace/bert/bert_config.json', \n",
    "                  'output_dir': 'opt/ml/model',\n",
    "                  'train_file': '/workspace/bert/data/squad/v1.1/train-v1.1.json',\n",
    "                  'num_gpus':8, 'train_batch_size':7, 'max_seq_length':512, 'doc_stride':128, 'seed':1,\n",
    "                  'learning_rate':3e-5,\n",
    "                  'save_to_s3':bucket}\n",
    "\n",
    "# instantiate model\n",
    "torch_model = PyTorch( role=role,\n",
    "                      train_instance_count=2,\n",
    "                      train_instance_type='ml.p3.16xlarge',\n",
    "                      entry_point='transform_script.py',\n",
    "                           image_name='209419068016.dkr.ecr.us-east-1.amazonaws.com/bert-ngc-torch-train',\n",
    "                          framework_version='1.4.0',\n",
    "                      hyperparameters=hyperparameters\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune the model\n",
    "\n",
    "If you use an instance with 4 GPUs and a batch size of 3 this process will take ~15 minutes to complete for this particular finetuning task with 2 epochs. Each additional epoch will add another 7 or so minutes. It's recommended to at minimum use a training instance with 4 GPUs, although you will likely get better performance with one of the ml.p3.16xlarge or ml.p3dn.24xlarge instances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-05 18:07:30 Starting - Starting the training job...\n",
      "2020-05-05 18:07:33 Starting - Launching requested ML instances.........\n",
      "2020-05-05 18:09:11 Starting - Preparing the instances for training......\n",
      "2020-05-05 18:10:25 Downloading - Downloading input data."
     ]
    }
   ],
   "source": [
    "torch_model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: bert-ngc-torch-testing-2020-05-01-03-56-26-630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------*"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error hosting endpoint bert-endpoint-byoc-test1: Failed. Reason:  The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint..",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-ee7ad0234bcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#model_data = 's3://privisaa-bucket-virginia/model.tar.gz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m bert_end = torch_model.deploy(instance_type='ml.g4dn.12xlarge', initial_instance_count=1, \n\u001b[0;32m----> 4\u001b[0;31m                       endpoint_name=endpoint_name)\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# try:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#     bert_end = torch_model.deploy(instance_type='ml.g4dn.12xlarge', initial_instance_count=1,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, accelerator_type, endpoint_name, use_compiled_model, update_endpoint, wait, model_name, kms_key, data_capture_config, tags, **kwargs)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mkms_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkms_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0mdata_capture_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_capture_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m         )\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/model.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, accelerator_type, endpoint_name, update_endpoint, tags, kms_key, wait, data_capture_config)\u001b[0m\n\u001b[1;32m    476\u001b[0m                 \u001b[0mkms_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkms_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m                 \u001b[0mdata_capture_config_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_capture_config_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m             )\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mendpoint_from_production_variants\u001b[0;34m(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict)\u001b[0m\n\u001b[1;32m   2847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2849\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2851\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand_role\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrole\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mcreate_endpoint\u001b[0;34m(self, endpoint_name, config_name, tags, wait)\u001b[0m\n\u001b[1;32m   2379\u001b[0m         )\n\u001b[1;32m   2380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2381\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2382\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mendpoint_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mwait_for_endpoint\u001b[0;34m(self, endpoint, poll)\u001b[0m\n\u001b[1;32m   2636\u001b[0m                 ),\n\u001b[1;32m   2637\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"InService\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2638\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2639\u001b[0m             )\n\u001b[1;32m   2640\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error hosting endpoint bert-endpoint-byoc-test1: Failed. Reason:  The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint.."
     ]
    }
   ],
   "source": [
    "endpoint_name = 'bert-endpoint-byoc-test1'\n",
    "#model_data = f's3://{bucket}/model.tar.gz'\n",
    "bert_end = torch_model.deploy(instance_type='ml.g4dn.12xlarge', initial_instance_count=1, \n",
    "                      endpoint_name=endpoint_name)\n",
    "# try:\n",
    "#     bert_end = torch_model.deploy(instance_type='ml.g4dn.12xlarge', initial_instance_count=1, \n",
    "#                           endpoint_name=endpoint_name)\n",
    "# except:\n",
    "    #print('deploy command failed, using backup method')\n",
    "#     torch_model = PyTorchModel(model_data=model_data,\n",
    "#                            role=role,\n",
    "#                           entry_point='transform_script.py',\n",
    "#                           framework_version='1.4.0')\n",
    "#     bert_end = torch_model.deploy(instance_type='ml.g4dn.8xlarge', initial_instance_count=1, \n",
    "#                               endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file='vocab'\n",
    "tokenizer = BertTokenizer(vocab_file, do_lower_case=True, max_len=512)\n",
    "max_seq_length, max_query_length, n_best_size, max_answer_length, null_score_diff_threshold= 384, 64, 1, 30, -11.0\n",
    "do_lower_case, can_give_negative_answer=True, True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-4b8437b1d34a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m                                            Body=payload.tobytes())\n\u001b[1;32m     23\u001b[0m answer = get_predictions(doc_tokens, tokens_for_postprocessing, \n\u001b[0;32m---> 24\u001b[0;31m                          \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_best_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                          \u001b[0mmax_answer_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                          \u001b[0mcan_give_negative_answer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "n_best_size=3\n",
    "context='Danielle is a girl who really loves her cat, Steve. Steve is a large cat with a very furry belly. He gets very excited by the prospect of eating chicken covered in gravy.'\n",
    "question='who loves Steve?'  # 'What kind of food does Steve like?'\n",
    "doc_tokens = context.split()\n",
    "query_tokens = tokenizer.tokenize(question)\n",
    "feature = preprocess_tokenized_text(doc_tokens, \n",
    "                                    query_tokens, \n",
    "                                    tokenizer, \n",
    "                                    max_seq_length=max_seq_length, \n",
    "                                    max_query_length=max_query_length)\n",
    "tensors_for_inference, tokens_for_postprocessing = feature\n",
    "\n",
    "input_ids = np.array(tensors_for_inference.input_ids, dtype=np.int64)\n",
    "segment_ids = np.array(tensors_for_inference.segment_ids, dtype=np.int64)\n",
    "input_mask = np.array(tensors_for_inference.input_mask, dtype=np.int64)   \n",
    "\n",
    "payload = np.concatenate([np.expand_dims(input_ids, axis=0), np.expand_dims(segment_ids, axis=0), np.expand_dims(input_mask, axis=0)])\n",
    "#response = bert_end.predict(payload.tobytes(), initial_args={'ContentType':'application/x-npy'}) \n",
    "response = runtime_client.invoke_endpoint(EndpointName='bert-endpoint-byoc-test',\n",
    "                                           ContentType='application/x-npy',\n",
    "                                           Body=payload.tobytes())\n",
    "answer = get_predictions(doc_tokens, tokens_for_postprocessing, \n",
    "                         response[0], response[1], n_best_size, \n",
    "                         max_answer_length, do_lower_case, \n",
    "                         can_give_negative_answer, \n",
    "                         null_score_diff_threshold)\n",
    "\n",
    "# print result\n",
    "print(f'{question} : {answer[0][\"text\"]}')\n",
    "print(f'inference took: {round(time.time()-t,4)} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_in_data = {'context':context, 'question':question}\n",
    "response = bert_end.predict(json.dump(pass_in_data), initial_args={'ContentType':'application/json'}) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
